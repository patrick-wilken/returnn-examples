#!rnn.py

# task
task = "train"
use_tensorflow = True
device = "gpu"

# datasets
EpochPartitions = 1
TrainSeqOrder = "laplace:30"
#TrainSeqOrder = "default"

def get_dataset(path, corpus):
    return {
        "name": path + "_" + corpus,
        "class": "TranslationDataset",
        "path": path,
        "file_postfix": corpus,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        "unknown_label": "<UNK>",
    }

train_en_de = get_dataset("dataset_en-de", "train")
train_en_fr = get_dataset("dataset_en-fr", "train")

train = {"class": "MetaDataset",
    "datasets": {"en-de": train_en_de, "en-fr": train_en_fr},
    "data_map": {"data": ("en-de", "data"),
        "classes_de": ("en-de", "classes"),
        "classes_fr": ("en-fr", "classes")},
    "partition_epoch": EpochPartitions,
    "seq_ordering": TrainSeqOrder,
}

dev_en_de = get_dataset("dataset_en-de", "test")
dev_en_fr = get_dataset("dataset_en-fr", "test")

dev = {"class": "MetaDataset",
    "datasets": {"en-de": dev_en_de, "en-fr": dev_en_fr},
    "data_map": {"data": ("en-de", "data"),
        "classes_de": ("en-de", "classes"),
        "classes_fr": ("en-fr", "classes")},
    "partition_epoch": 1,
    "seq_ordering": "sorted",
}

test = dev # Obviously, don't do this for real experiments :)

one_en_de = get_dataset("dataset_en-de", "one")
one_en_fr = get_dataset("dataset_en-fr", "one")

one = {"class": "MetaDataset",
    "datasets": {"en-de": one_en_de, "en-fr": one_en_fr},
    "data_map": {"data": ("en-de", "data"),
        "classes_de": ("en-de", "classes"),
        "classes_fr": ("en-fr", "classes")},
    "partition_epoch": 1,
    "seq_ordering": "sorted",
}

# network
SourceVocSize = 2732
TargetVocSizeDe = 3258
TargetVocSizeFr = 2911

EmbeddingSize = 53
HiddenSize = 94

BeamSize = 5
SearchMaxSequenceLength = "max_len_from('encoder') * 3"

network = {
    "combine_classes": {"class": "eval", "from": ["data:classes_de", "data:classes_fr"],
        "eval": "source(0) * {} + source(1)".format(TargetVocSizeFr), "register_as_extern_data": "classes_combined",
        "out_type": {"dim": TargetVocSizeDe * TargetVocSizeFr, "sparse": True}},

    "source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": EmbeddingSize},

    "lstm_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : HiddenSize, "direction": 1, "from": ["source_embed"] },

    "lstm_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : HiddenSize, "direction": 1, "from": ["source_embed"] },
    "lstm_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : HiddenSize, "direction": -1, "from": ["source_embed"] },

    "encoder": {"class": "copy", "from": ["lstm_fw", "lstm_bw"]},
    "enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": HiddenSize},
    "inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

    "output": {"class": "rec", "from": [], "unit": {
        "end": {"class": "compare", "from": ["output_de"], "value": 0},
        "target_embed": {"class": "linear", "activation": None, "with_bias": False, "from": ["output_de"], "n_out": EmbeddingSize, "initial_output": 0},
        "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": HiddenSize},
        "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 2 * HiddenSize},
        "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": HiddenSize},
        "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "prev_s_transformed"], "n_out": HiddenSize},
        "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
        "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},
        "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},
        "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
            "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": 1, "shape": (None, 1)}},
        "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},
        "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": HiddenSize},
        "readout_in": {"class": "linear", "from": ["prev:s", "prev:target_embed", "att"], "activation": None, "n_out": HiddenSize},
        "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]},

        "output_prob_de": {
            "class": "softmax", "from": ["readout"], "dropout": 0.3,
            "target": "classes_de", "loss": "ce", "loss_opts": {"label_smoothing": 0.1}},
        "output_prob_fr": {
            "class": "softmax", "from": ["readout"], "dropout": 0.3,
            "target": "classes_fr", "loss": "ce", "loss_opts": {"label_smoothing": 0.1}},

        "output_prob_de_expanded": {
            "class": "expand_dims", "from": ["output_prob_de"], "axis": "feature"},
        "output_prob_fr_expanded": {
            "class": "expand_dims", "from": ["output_prob_fr"], "axis": "feature"},

        "output_prob_combined_matrix": {
            "class": "dot", "from": ["output_prob_de_expanded", "output_prob_fr_expanded"], "red2": -1, "var2": -2, "debug": True},

        "output_prob_combined": {
            "class": "merge_dims", "from": ["output_prob_combined_matrix"], "axes": "except_batch"},

        "output": {"class": "choice", "target": "classes_combined", "from": ["output_prob_combined"], "beam_size": BeamSize, "initial_output": 0},
        "output_de": {"class": "eval", "from": ["output"], "eval": "tf.floordiv(x=source(0), y={})".format(TargetVocSizeFr), "out_type": {
            "shape": (),  #### This line has to be commented for training! :( ####
            "dim": TargetVocSizeDe, "sparse": True,
        }},

    }, "target": "classes_combined", "max_seq_len": SearchMaxSequenceLength},

    "decision": {
        "class": "decide", "from": ["output"], "loss": "edit_distance", "target": "classes_combined",
    },

    "decision_de": {"class": "eval", "from": ["decision"], "eval": "tf.floordiv(x=source(0), y={})".format(TargetVocSizeFr),
        "loss": "edit_distance", "target": "classes_de", "out_type": {"dim": TargetVocSizeDe, "sparse": True}},
    "decision_fr": {"class": "eval", "from": ["decision"], "eval": "tf.floormod(x=source(0), y={})".format(TargetVocSizeFr),
        "loss": "edit_distance", "target": "classes_fr", "out_type": {"dim": TargetVocSizeFr, "sparse": True}},
}

num_outputs = {"data": [SourceVocSize, 1], "classes_de": [TargetVocSizeDe, 1], "classes_fr": [TargetVocSizeFr, 1]}
num_inputs = SourceVocSize

# training
num_epochs = 100
model = "model/params"

adam = True
batch_size = 0
max_seqs = 11

learning_rate = 0.001
learning_rate_control = "newbob_multi_epoch"
learning_rate_file = "model/newbob.data"

# search
search_output_layer = ["decision_de", "decision_fr"]
search_data = one
#search_data = test
search_output_file = "hyp"

# logging
debug_print_layer_output_template = True
debug_print_layer_output_shape = False
log_verbosity = 5

