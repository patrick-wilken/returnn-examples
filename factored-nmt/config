#!rnn.py

# task
task = "train"
use_tensorflow = True
device = "gpu"

# datasets
EpochPartitions = 1
TrainSeqOrder = "laplace:30"

def get_dataset(path, corpus):
    return {
        "name": path + "_" + corpus,
        "class": "TranslationDataset",
        "path": path,
        "file_postfix": corpus,
        "source_postfix": " </S>",
        "target_postfix": " </S>",
        "unknown_label": "<UNK>",
    }

train_en_de = get_dataset("dataset_en-de", "train")
train_en_fr = get_dataset("dataset_en-fr", "train")

train = {"class": "MetaDataset",
    "datasets": {"en-de": train_en_de, "en-fr": train_en_fr},
    "data_map": {"data": ("en-de", "data"),
        "classes_de": ("en-de", "classes"),
        "classes_fr": ("en-fr", "classes")},
    "partition_epoch": EpochPartitions,
    "seq_ordering": TrainSeqOrder,
}

# to check if we can overfit on toy data
dev = train
test = train

# network
SourceVocSize = 2732
TargetVocSizeDe = 3258
TargetVocSizeFr = 2911

EmbeddingSize = 200
HiddenSize = 200

BeamSize = 12
BeamSizeDe = 6
BeamSizeFr = 6
SearchMaxSequenceLength = "max_len_from('encoder') * 3"

network = {
    "source_embed": {"class": "linear", "activation": None, "with_bias": False, "n_out": EmbeddingSize},

    "lstm_fw" : { "class": "rec", "unit": "nativelstm2", "n_out" : HiddenSize, "direction": 1, "from": ["source_embed"] },
    "lstm_bw" : { "class": "rec", "unit": "nativelstm2", "n_out" : HiddenSize, "direction": -1, "from": ["source_embed"] },

    "encoder": {"class": "copy", "from": ["lstm_fw", "lstm_bw"]},
    "enc_ctx": {"class": "linear", "activation": None, "with_bias": True, "from": ["encoder"], "n_out": HiddenSize},
    "inv_fertility": {"class": "linear", "activation": "sigmoid", "with_bias": False, "from": ["encoder"], "n_out": 1},

    "output": {"class": "rec", "from": [], "unit": {
        "end": {"class": "compare", "from": ["output_de"], "value": 0}, # "from": ["output/out_0"] would also work
        "target_embed": {"class": "linear", "activation": None, "with_bias": False, "from": ["output_de"], "n_out": EmbeddingSize, "initial_output": 0},
        "weight_feedback": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev:accum_att_weights"], "n_out": HiddenSize},
        "prev_s_state": {"class": "get_last_hidden_state", "from": ["prev:s"], "n_out": 2 * HiddenSize},
        "prev_s_transformed": {"class": "linear", "activation": None, "with_bias": False, "from": ["prev_s_state"], "n_out": HiddenSize},
        "energy_in": {"class": "combine", "kind": "add", "from": ["base:enc_ctx", "weight_feedback", "prev_s_transformed"], "n_out": HiddenSize},
        "energy_tanh": {"class": "activation", "activation": "tanh", "from": ["energy_in"]},
        "energy": {"class": "linear", "activation": None, "with_bias": False, "from": ["energy_tanh"], "n_out": 1},
        "att_weights": {"class": "softmax_over_spatial", "from": ["energy"]},
        "accum_att_weights": {"class": "eval", "from": ["prev:accum_att_weights", "att_weights", "base:inv_fertility"],
            "eval": "source(0) + source(1) * source(2) * 0.5", "out_type": {"dim": 1, "shape": (None, 1)}},
        "att": {"class": "generic_attention", "weights": "att_weights", "base": "base:encoder"},
        "s": {"class": "rnn_cell", "unit": "LSTMBlock", "from": ["target_embed", "att"], "n_out": HiddenSize},
        "readout_in": {"class": "linear", "from": ["prev:s", "prev:target_embed", "att"], "activation": None, "n_out": HiddenSize},
        "readout": {"class": "reduce_out", "mode": "max", "num_pieces": 2, "from": ["readout_in"]},

        "output_prob_de": {"class": "softmax", "from": ["readout"], "dropout": 0.3, "target": "classes_de", "loss": "ce",
            "loss_opts": {"label_smoothing": 0.1}},
        "output_prob_fr": {"class": "softmax", "from": ["readout"], "dropout": 0.3, "target": "classes_fr", "loss": "ce",
            "loss_opts": {"label_smoothing": 0.1}},
        "output": {"class": "choice", "from": ["output_prob_de", "output_prob_fr"], "target": ["classes_de", "classes_fr"],
            "beam_size": BeamSize, "source_beam_sizes": [BeamSizeDe, BeamSizeFr],
        },

        # Give proper names to out_0 and out_1.
        # Also, we have to explicitly make out_1 an output layer (as it is not used inside the recurrent unit).
        "output_de": {"class": "copy", "from": ["output/out_0"], "is_output_layer": True},
        "output_fr": {"class": "copy", "from": ["output/out_1"], "is_output_layer": True},

    }, "target": "classes_de", "max_seq_len": SearchMaxSequenceLength},

    # target only needed here if used as search_output_layer
    "output_de": {"class": "get_rec_accumulated", "from": ["output"], "sub_layer": "output_de", "target": "classes_de"},
    "output_fr": {"class": "get_rec_accumulated", "from": ["output"], "sub_layer": "output_fr", "target": "classes_fr"},

    "decision_de": {"class": "decide", "from": ["output_de"], "loss": "edit_distance", "target": "classes_de"},
    "decision_fr": {"class": "decide", "from": ["output_fr"], "loss": "edit_distance", "target": "classes_fr"},
}

num_outputs = {"data": [SourceVocSize, 1], "classes_de": [TargetVocSizeDe, 1], "classes_fr": [TargetVocSizeFr, 1]}
num_inputs = SourceVocSize

# training
num_epochs = 100
model = "model/params"

adam = True
batch_size = 0
max_seqs = 11

learning_rate = 0.001
learning_rate_control = "newbob_multi_epoch"
learning_rate_file = "model/newbob.data"

# search
#search_output_layer = ["output_de", "output_fr"]
search_output_layer = ["decision_de", "decision_fr"]
search_data = test
search_output_file = "hyp"
search_output_file_format = "py"

# logging
debug_print_layer_output_template = True
debug_print_layer_output_shape = False
log_verbosity = 5

